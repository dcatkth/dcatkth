{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flowing code trains a Deep Neural Network (DNN) for classification with two hidden layers (one with 300\n",
    "neurons, and the other with 100 neurons) and a softmax output layer with 10 neurons. We run this code on the MNIST dataset. Moreover, we apply the some changes (listed below) to the network to tackle the following challenges:\n",
    "* Overfitting: the l1 regularization\n",
    "* Vanishing gradinet: the He initilization, ELU activation function in hidden layers, and Batch Normalization at each layer\n",
    "* Training speed: Adam optimization\n",
    "\n",
    "In this code, we also use the **learning rate scheduling** technique to update the learning reate over exeuting the model. Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge, and if you set it too low, training will eventually converge to the optimum, but it will take a very long time. In this example we use the exponential scheduling to update the learning rate. To do so, after setting the hyperparameter values, we create a nontrainable variable `global_step` (initialized to 0) to keep track of the current training iteration number. Then we define an exponentially decaying learning rate using TensorFlow's `exponential_decay()` function. Next, we create an optimizer using this decaying learning rate. Finally, we create the training operation by calling the optimizer's `minimize()` method; since we pass it the `global_step` variable, it will kindly take care of incrementing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual building layers\n",
    "reset_graph()\n",
    "\n",
    "n_features = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "batch_norm_momentum = 0.9\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "########################################\n",
    "# load the mnist dataset\n",
    "########################################\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "\n",
    "########################################\n",
    "# define the placeholders\n",
    "########################################\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "########################################\n",
    "# build the network (using l1-regularization and batch normalizatio)\n",
    "########################################\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    scale = 0.001\n",
    "\n",
    "    # he initilization\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, kernel_initializer=he_init, \n",
    "                              kernel_regularizer=tf.contrib.layers.l1_regularizer(scale), name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, kernel_initializer=he_init, \n",
    "                              kernel_regularizer=tf.contrib.layers.l1_regularizer(scale), name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, kernel_initializer=he_init, \n",
    "                              kernel_regularizer=tf.contrib.layers.l1_regularizer(scale), name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "########################################\n",
    "# define the cost (loss) function\n",
    "########################################\n",
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)                                # not shown\n",
    "    base_cost = tf.reduce_mean(cross_entropy, name=\"avg_xentropy\")\n",
    "    reg_cost = tf.losses.get_regularization_loss()\n",
    "    cost = base_cost + reg_cost\n",
    "\n",
    "########################################\n",
    "# train the model (use  learning rate scheduling)\n",
    "########################################\n",
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1 / 10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "########################################\n",
    "# define the evaluation metrics\n",
    "########################################\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# since we are using tf.layers.batch_normalization(), we need to explicitly run the extra update operations\n",
    "# needed by batch normalization\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the dropout technique instead of l1-regularization to solve the overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual building layers\n",
    "reset_graph()\n",
    "\n",
    "n_features = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "batch_norm_momentum = 0.9\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "dropout_rate = 0.5\n",
    "\n",
    "########################################\n",
    "# load the mnist dataset\n",
    "########################################\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "\n",
    "########################################\n",
    "# define the placeholders\n",
    "########################################\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "########################################\n",
    "# build the network (using l1-regularization and batch normalizatio)\n",
    "########################################\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "    # he initilization\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, kernel_initializer=he_init, name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=batch_norm_momentum)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden1_drop = tf.layers.dropout(bn1_act, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, kernel_initializer=he_init, name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=batch_norm_momentum)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    hidden2_drop = tf.layers.dropout(bn2_act, dropout_rate, training=training)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(hidden2_drop, n_outputs, kernel_initializer=he_init, name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=batch_norm_momentum)\n",
    "\n",
    "########################################\n",
    "# define the cost (loss) function\n",
    "########################################\n",
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)                                # not shown\n",
    "    cost = tf.reduce_mean(cross_entropy, name=\"cost\")\n",
    "\n",
    "########################################\n",
    "# train the model (use  learning rate scheduling)\n",
    "########################################\n",
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1 / 10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "########################################\n",
    "# define the evaluation metrics\n",
    "########################################\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# since we are using tf.layers.batch_normalization(), we need to explicitly run the extra update operations\n",
    "# needed by batch normalization\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
